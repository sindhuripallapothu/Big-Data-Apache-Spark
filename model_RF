from pyspark.sql import SQLContext
from pyspark import SparkContext
from pyspark.sql.functions import col
from pyspark.ml import Pipeline
from pyspark.ml.feature import *
from pyspark.ml.classification import *
from pyspark.ml.evaluation import *
from kafka import KafkaConsumer
import pandas as pd
from nltk.corpus import stopwords

sc = SparkContext()
sqlContext = SQLContext(sc)


def TextToDF(url):
    df = pd.read_csv(url, delimiter='\n', header=None)
    df.columns = ['News']

    df["category"] = df["News"].str[:1]
    df["News"] = df["News"].str[3:]
    df["category"] = df["category"].astype(int)
    print(df)

    df_spark = sqlContext.createDataFrame(df, schema=None)

    return df_spark


train_data = TextToDF(r"/home/sindhuri/Downloads/outFile_training.txt")
# print(type(train_data))

stop_word = list(set(stopwords.words('english')))

regTokenizer = RegexTokenizer(inputCol = "News", outputCol= "words", pattern="\\W")
remover = StopWordsRemover(inputCol="words", outputCol="filtered").setStopWords(stop_word)
htf = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=10000)
idf = IDF(inputCol="rawFeatures", outputCol= "features", minDocFreq=5)
label_string_indexer = StringIndexer(inputCol="category", outputCol="label")
label_string_indexer.setHandleInvalid("keep")

pipeline = Pipeline(stages=[regTokenizer, remover, htf, idf, label_string_indexer])
pipelinefit = pipeline.fit(train_data)
dataset = pipelinefit.transform(train_data)
print("dataset.show(5)")
dataset.show(5)

#(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)

rf = RandomForestClassifier(labelCol= "label",
                            featuresCol= "features",
                            numTrees = 100)
rfModel = rf.fit(dataset)

consumer = KafkaConsumer('guardian2', bootstrap_servers=['localhost:9092'],
                         auto_offset_reset = 'earliest',
                         enable_auto_commit = True,
                         api_version =(0,10),
                         consumer_timeout_ms=10000,
                         value_deserializer=lambda x: x.decode('utf-8'))
print("Starting ML prediction")
data = []
for message in consumer:
    tmp = list()
    tmp.append(int(message.value.split("||")[0]))
    tmp.append(message.value.split("||")[1])
    data.append(tmp)

df_spark_test = sqlContext.createDataFrame(data, schema= ["category", "News"])
dataset_test = pipelinefit.transform(df_spark_test)
predictions = rfModel.transform(dataset_test)
predictions.filter(predictions['prediction'] == 0)\
    .select("News", "category", "probability", "label", "prediction", "features")\
    .orderBy("probability", ascending = False)\
    .show(5)
evaluator = MulticlassClassificationEvaluator(predictionCol = "prediction")
accuracy = evaluator.evaluate(predictions)
print(accuracy*100)
